- step:
    name: dino-train
    image: neurolabszia.azurecr.io/valohai-zia-vision:phase3-multi-gpu-training-gpu
    command:
      - mkdir /valohai/inputs/data/
      - mkdir /valohai/inputs/model/
      - mkdir /valohai/outputs/models/
      - python untar_archives.py --input_path /valohai/inputs/data
      - python convert_coco_to_image_folder.py --coco_dataset_path /valohai/inputs/data/dataset/ --coco_json_filename coco_train.json --image_folder_output_path /valohai/inputs/data/dataset
      - python main_dino.py {parameters}
    inputs:
      - name: data
        default: gs://valohai-datasets/syn/3dmodels/fruits-and-friends-revised.tar.gz

    parameters:
      - name: epochs
        type: integer
        pass-as: --epochs={v}
        default: 100
        description: Number of epochs of training
      - name: lr
        type: float
        pass-as: --lr={v}
        default: 0.0005
        description: learning rate at the end of the linear warmup.
      - name: min-lr
        type: float
        pass-as: --min_lr={v}
        default: 0.000001
        description: Target LR at the end of optimization. We use a cosine LR schedule with linear warmup    
      - name: optimizer
        type: string
        pass-as: --optimizer={v}
        default: adamw
        description: Type of optimizer. We recommend using adamw with ViTs. Choices [adamw, sgd, lars].
      - name: data-path
        type: string
        pass-as: --data_path={v}
        description: path to input data.
        default: /valohai/inputs/data/dataset
      - name: output-dir
        type: string
        pass-as: --output_dir={v}
        description: path to output directory.
      - name: architecture
        type: string
        pass-as: --arch={v}
        default: vit_small
        description: model architecture used for training.
      - name: patch-size
        type: integer
        pass-as: --patch_size={v}
        default: 16
        description: Size in pixels of input square patches - default 16 (for 16x16 patches)
      - name: output-dim
        type: integer
        pass-as: --out_dim={v}
        default: 65536
        description: Dimensionality of the DINO head output.

- step:
    name: convert_coco_to_image_folder
    image: neurolabszia.azurecr.io/valohai-zia-vision:phase3-multi-gpu-training-gpu
    command:
      - tar -xf /valohai/inputs/data/dataset/*.tar.gz -C /valohai/inputs/data/dataset/
      - python convert_coco_to_image_folder.py {parameters}

    inputs:
      - name: dataset
        default: gs://valohai-datasets/syn/3dmodels/fruits-and-friends-revised.tar.gz
    
    parameters:
      - name: Data path
        type: string
        pass-as: --coco_dataset_path={v}
        description: path to input data.
        default: /valohai/inputs/data/dataset/dataset/
      - name: Dataset type
        type: string
        pass-as: --dataset_type={v}
        description: train or test dataset
        default: train
      - name: Image folder output folder
        type: string
        pass-as: --image_folder_output_path={v}
        description: Output folder of VH
        default: train
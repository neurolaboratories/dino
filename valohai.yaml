- step:
    name: dino-train
    image: neurolabszia.azurecr.io/valohai-zia-vision:phase3-multi-gpu-training-gpu
    command:
      - mkdir /valohai/inputs/data/
      - mkdir /valohai/inputs/model/
      - mkdir /valohai/outputs/models/
      - python untar_archives.py --input_path /valohai/inputs/data
      - python -m torch.distributed.launch --nproc_per_node={parameter-value:num_gpus} main_dino.py {parameters}
    
    inputs:
      - name: data
        default: gs://valohai-datasets/syn/3dmodels/fruits-and-friends-revised.tar.gz
    parameters:
      - name: epochs
        type: integer
        pass-as: --epochs={v}
        default: 100
        description: Number of epochs of training
      - name: warmup-epochs
        type: integer
        pass-as: --warmup_epochs={v}
        default: 10
        description: Number of epochs of training
      - name: lr
        type: float
        pass-as: --lr={v}
        default: 0.0005
        description: learning rate at the end of the linear warmup.
      - name: min-lr
        type: float
        pass-as: --min_lr={v}
        default: 0.000001
        description: Target LR at the end of optimization. We use a cosine LR schedule with linear warmup    
      - name: optimizer
        type: string
        pass-as: --optimizer={v}
        default: adamw
        description: Type of optimizer. We recommend using adamw with ViTs. Choices [adamw, sgd, lars].
      - name: data-path
        type: string
        pass-as: --data_path={v}
        description: path to input data.
        default: /valohai/inputs/data/dataset/train
      - name: output-dir
        type: string
        pass-as: --output_dir={v}
        description: path to output directory.
        default: /valohai/outputs/
      - name: architecture
        type: string
        pass-as: --arch={v}
        default: vit_small
        description: model architecture used for training.
      - name: patch-size
        type: integer
        pass-as: --patch_size={v}
        default: 16
        description: Size in pixels of input square patches - default 16 (for 16x16 patches)
      - name: output-dim
        type: integer
        pass-as: --out_dim={v}
        default: 65536
        description: Dimensionality of the DINO head output.
      - name: batch_size_per_gpus
        type: integer
        pass-as: --batch_size_per_gpu={v}
        description: Batch size per GPU.
      - name: num_gpus
        type: integer
        default: 1
        description: number of gpus for distributed training
- step:
    name: convert_coco_to_image_folder
    image: neurolabszia.azurecr.io/valohai-zia-vision:phase3-multi-gpu-training-gpu
    command:
      - tar -xf /valohai/inputs/data/dataset/*.tar.gz -C /valohai/inputs/data/dataset/
      - python convert_coco_to_image_folder.py {parameters}
      - tar -czvf /valohai/outputs/output_image_folder.tar.gz {parameter-value:img_folder_output_path}* --remove-files

    inputs:
      - name: dataset
        default: gs://valohai-datasets/syn/3dmodels/fruits-and-friends-revised.tar.gz
    
    parameters:
      - name: data_path
        type: string
        pass-as: --coco_dataset_path={v}
        description: path to input data.
        default: /valohai/inputs/data/dataset/dataset/
      - name: dataset_type
        type: string
        pass-as: --dataset_type={v}
        description: train or test dataset
        default: train
      - name: img_folder_output_path
        type: string
        pass-as: --image_folder_output_path={v}
        description: Output folder of VH
        default: /valohai/outputs/train_data/